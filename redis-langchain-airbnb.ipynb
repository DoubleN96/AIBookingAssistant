{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yk3jkeRp3Dd-",
    "outputId": "2b352d3a-1598-4727-b25e-714fc705d0f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import redis\n",
    "import time\n",
    "\n",
    "from redis.commands.search.field import VectorField\n",
    "from redis.commands.search.field import TextField\n",
    "from redis.commands.search.field import TagField\n",
    "from redis.commands.search.query import Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "redis_conn = redis.Redis(\n",
    "  host='localhost',\n",
    "  port=6379,\n",
    "  password='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "00_n4VWH7FoB",
    "outputId": "f26daa8c-4af9-4def-d5ab-3197777fe2f9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_TEXT_LENGTH=512\n",
    "NUMBER_PRODUCTS=10000\n",
    "\n",
    "def auto_truncate(val):\n",
    "    return str(val)[:MAX_TEXT_LENGTH]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36494/902267153.py:1: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  room_data = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "room_data = pd.read_csv(\n",
    "    'app/data/airbnb.csv', delimiter=',', encoding_errors='ignore', on_bad_lines='skip',\n",
    "    converters={'bullet_point': auto_truncate,'amenities':auto_truncate,'name':auto_truncate}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "room_data = room_data[\n",
    "    [\n",
    "        'listing_id', 'name', 'host_id', 'host_since', 'host_location',\n",
    "       'host_is_superhost', 'host_total_listings_count',\n",
    "       'host_has_profile_pic', 'host_identity_verified', 'neighbourhood',\n",
    "       'district', 'city', 'property_type',\n",
    "       'room_type', 'accommodates', 'bedrooms', 'amenities', 'price',\n",
    "       'minimum_nights', 'maximum_nights', 'review_scores_rating',\n",
    "       'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "       'review_scores_checkin', 'review_scores_communication',\n",
    "       'review_scores_location', 'review_scores_value', 'instant_bookable'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# room_data = room_data.drop_duplicates(subset=['Area'], keep='first', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "room_data = room_data.sample(10000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "listing_id                                                              42373224\n",
       "name                                                                 Santo House\n",
       "host_id                                                                186234610\n",
       "host_since                                                            2018-04-25\n",
       "host_location                                                  Bangkok, Thailand\n",
       "host_is_superhost                                                              f\n",
       "host_total_listings_count                                                    2.0\n",
       "host_has_profile_pic                                                           t\n",
       "host_identity_verified                                                         f\n",
       "neighbourhood                                                        Phra Nakhon\n",
       "district                                                                     NaN\n",
       "city                                                                     Bangkok\n",
       "property_type                                  Private room in bed and breakfast\n",
       "room_type                                                           Private room\n",
       "accommodates                                                                   2\n",
       "bedrooms                                                                     6.0\n",
       "amenities                      [\"Air conditioning\", \"Lock on bedroom door\", \"...\n",
       "price                                                                        600\n",
       "minimum_nights                                                                 1\n",
       "maximum_nights                                                              1125\n",
       "review_scores_rating                                                       100.0\n",
       "review_scores_accuracy                                                      10.0\n",
       "review_scores_cleanliness                                                   10.0\n",
       "review_scores_checkin                                                       10.0\n",
       "review_scores_communication                                                  8.0\n",
       "review_scores_location                                                      10.0\n",
       "review_scores_value                                                          8.0\n",
       "instant_bookable                                                               t\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "room_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get the first 1000 products with non-empty item keywords\n",
    "rooms_metadata = room_data.head(NUMBER_PRODUCTS).to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such component or layout: user_msg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     messages\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: transcript[\u001b[39m\"\u001b[39m\u001b[39muser_msg\u001b[39m\u001b[39m\"\u001b[39m]})\n\u001b[1;32m     15\u001b[0m \u001b[39m## demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m ui \u001b[39m=\u001b[39m gr\u001b[39m.\u001b[39;49mInterface(fn\u001b[39m=\u001b[39;49mtranscribe, inputs\u001b[39m=\u001b[39;49mgr\u001b[39m.\u001b[39;49mAudio(source\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmicrophone\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfilepath\u001b[39;49m\u001b[39m\"\u001b[39;49m), outputs\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39muser_msg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mlaunch()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gradio/interface.py:247\u001b[0m, in \u001b[0;36mInterface.__init__\u001b[0;34m(self, fn, inputs, outputs, examples, cache_examples, examples_per_page, live, interpretation, num_shap, title, description, article, thumbnail, theme, css, allow_flagging, flagging_options, flagging_dir, flagging_callback, analytics_enabled, batch, max_batch_size, _api_mode, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_examples \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_components \u001b[39m=\u001b[39m [\n\u001b[1;32m    245\u001b[0m     get_component_instance(i, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m    246\u001b[0m ]\n\u001b[0;32m--> 247\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_components \u001b[39m=\u001b[39m [\n\u001b[1;32m    248\u001b[0m     get_component_instance(o, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m outputs\n\u001b[1;32m    249\u001b[0m ]\n\u001b[1;32m    251\u001b[0m \u001b[39mfor\u001b[39;00m component \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_components \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_components:\n\u001b[1;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(component, IOComponent)):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gradio/interface.py:248\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_examples \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_components \u001b[39m=\u001b[39m [\n\u001b[1;32m    245\u001b[0m     get_component_instance(i, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m    246\u001b[0m ]\n\u001b[1;32m    247\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_components \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 248\u001b[0m     get_component_instance(o, render\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m outputs\n\u001b[1;32m    249\u001b[0m ]\n\u001b[1;32m    251\u001b[0m \u001b[39mfor\u001b[39;00m component \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_components \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_components:\n\u001b[1;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(component, IOComponent)):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gradio/components.py:5810\u001b[0m, in \u001b[0;36mget_component_instance\u001b[0;34m(comp, render)\u001b[0m\n\u001b[1;32m   5808\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_component_instance\u001b[39m(comp: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mdict\u001b[39m \u001b[39m|\u001b[39m Component, render\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Component:\n\u001b[1;32m   5809\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(comp, \u001b[39mstr\u001b[39m):\n\u001b[0;32m-> 5810\u001b[0m         component_obj \u001b[39m=\u001b[39m component(comp)\n\u001b[1;32m   5811\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (render):\n\u001b[1;32m   5812\u001b[0m             component_obj\u001b[39m.\u001b[39munrender()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gradio/components.py:5802\u001b[0m, in \u001b[0;36mcomponent\u001b[0;34m(cls_name)\u001b[0m\n\u001b[1;32m   5801\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomponent\u001b[39m(cls_name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Component:\n\u001b[0;32m-> 5802\u001b[0m     obj \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mcomponent_or_layout_class(cls_name)()\n\u001b[1;32m   5803\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, BlockContext):\n\u001b[1;32m   5804\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid component: \u001b[39m\u001b[39m{\u001b[39;00mobj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gradio/utils.py:498\u001b[0m, in \u001b[0;36mcomponent_or_layout_class\u001b[0;34m(cls_name)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m cls_name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    494\u001b[0m         \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, gradio\u001b[39m.\u001b[39mcomponents\u001b[39m.\u001b[39mComponent)\n\u001b[1;32m    495\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, gradio\u001b[39m.\u001b[39mblocks\u001b[39m.\u001b[39mBlockContext)\n\u001b[1;32m    496\u001b[0m     ):\n\u001b[1;32m    497\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\n\u001b[0;32m--> 498\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo such component or layout: \u001b[39m\u001b[39m{\u001b[39;00mcls_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No such component or layout: user_msg"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "\n",
    "def transcribe(audio):\n",
    "    global messages\n",
    "\n",
    "    audio_file= open(audio, \"rb\")\n",
    "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "    print(transcript)\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": transcript[\"text\"]})\n",
    "\n",
    "## demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
    "ui = gr.Interface(fn=transcribe, inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), outputs=\"text\").launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gjouwTqF7ftf",
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m item_keywords \u001b[39m=\u001b[39m  [\n\u001b[1;32m      8\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(rooms_metadata[i][key]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhost_location\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneighbourhood\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mproperty_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mamenities\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m rooms_metadata\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     11\u001b[0m room_context \u001b[39m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     rooms_metadata[i][\u001b[39m'\u001b[39m\u001b[39mlisting_id\u001b[39m\u001b[39m'\u001b[39m]:\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(rooms_metadata[i][key]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhost_location\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneighbourhood\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mproperty_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mamenities\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m rooms_metadata\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m item_keywords_vectors \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39mencode(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m item_keywords]\n",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m item_keywords \u001b[39m=\u001b[39m  [\n\u001b[1;32m      8\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(rooms_metadata[i][key]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhost_location\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneighbourhood\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mproperty_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mamenities\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m rooms_metadata\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     11\u001b[0m room_context \u001b[39m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     rooms_metadata[i][\u001b[39m'\u001b[39m\u001b[39mlisting_id\u001b[39m\u001b[39m'\u001b[39m]:\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(rooms_metadata[i][key]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhost_location\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneighbourhood\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mproperty_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mamenities\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m rooms_metadata\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m item_keywords_vectors \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39;49mencode(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m item_keywords]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    845\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    853\u001b[0m     embedding_output,\n\u001b[1;32m    854\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    855\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    856\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    857\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    858\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    859\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    860\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    861\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    862\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    863\u001b[0m )\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    865\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    528\u001b[0m         hidden_states,\n\u001b[1;32m    529\u001b[0m         attention_mask,\n\u001b[1;32m    530\u001b[0m         layer_head_mask,\n\u001b[1;32m    531\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    532\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    533\u001b[0m         past_key_value,\n\u001b[1;32m    534\u001b[0m         output_attentions,\n\u001b[1;32m    535\u001b[0m     )\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:411\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    400\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    401\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    409\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    412\u001b[0m         hidden_states,\n\u001b[1;32m    413\u001b[0m         attention_mask,\n\u001b[1;32m    414\u001b[0m         head_mask,\n\u001b[1;32m    415\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    416\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    418\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    420\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:338\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    329\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    330\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    337\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 338\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    339\u001b[0m         hidden_states,\n\u001b[1;32m    340\u001b[0m         attention_mask,\n\u001b[1;32m    341\u001b[0m         head_mask,\n\u001b[1;32m    342\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    343\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    344\u001b[0m         past_key_value,\n\u001b[1;32m    345\u001b[0m         output_attentions,\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    347\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    348\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:217\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    215\u001b[0m     value_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey(hidden_states))\n\u001b[1;32m    218\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    220\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "#from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
    "\n",
    "\n",
    "item_keywords =  [\n",
    "    ', '.join(str(rooms_metadata[i][key]) for key in ['name', 'host_location', 'neighbourhood', 'property_type', 'amenities', 'price'])\n",
    "    for i in rooms_metadata.keys()\n",
    "]\n",
    "room_context = {\n",
    "    rooms_metadata[i]['listing_id']:', '.join(str(rooms_metadata[i][key]) for key in ['name', 'host_location', 'neighbourhood', 'property_type', 'amenities', 'price'])\n",
    "    for i in rooms_metadata.keys()\n",
    "}\n",
    "item_keywords_vectors = [model.encode(sentence) for sentence in item_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_keywords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iw7rlppY8f3a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(item_keywords_vectors)\n",
    "len(rooms_metadata)\n",
    "# Check one of the products\n",
    "rooms_metadata[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayGF0v-w8mxy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_vectors(client, product_metadata, vector_dict, vector_field_name):\n",
    "    p = client.pipeline(transaction=False)\n",
    "    for index in product_metadata.keys():    \n",
    "        #hash key\n",
    "        key='product_ID:'+ str(product_metadata[index]['listing_id'])\n",
    "        \n",
    "        #hash values\n",
    "        item_metadata = product_metadata[index]\n",
    "        item_keywords_vector = vector_dict[index].astype(np.float32).tobytes()\n",
    "        item_metadata[vector_field_name]=item_keywords_vector\n",
    "        \n",
    "        # HSET\n",
    "        p.hset(key,mapping=item_metadata)\n",
    "            \n",
    "    p.execute()\n",
    "\n",
    "def create_flat_index (redis_conn,vector_field_name,number_of_vectors, vector_dimensions=512, distance_metric='L2'):\n",
    "    redis_conn.ft().create_index([\n",
    "        VectorField(vector_field_name, \"FLAT\", {\"TYPE\": \"FLOAT32\", \"DIM\": vector_dimensions, \"DISTANCE_METRIC\": distance_metric, \"INITIAL_CAP\": number_of_vectors, \"BLOCK_SIZE\":number_of_vectors }),\n",
    "        TextField(\"property_type\", as_name='property_type'),\n",
    "        TextField(\"name\", as_name='name'),\n",
    "        TextField(\"amenities\", as_name='amenities'),\n",
    "        TextField(\"city\", as_name='city')\n",
    "    ]) \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcjo2rXb8pT9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ITEM_KEYWORD_EMBEDDING_FIELD='item_keyword_vector'\n",
    "TEXT_EMBEDDING_DIMENSION=768\n",
    "NUMBER_PRODUCTS=10000\n",
    "\n",
    "print ('Loading and Indexing + ' +  str(NUMBER_PRODUCTS) + ' products')\n",
    "\n",
    "#flush all data\n",
    "redis_conn.flushall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#create flat index & load vectors\n",
    "create_flat_index(\n",
    "    redis_conn, ITEM_KEYWORD_EMBEDDING_FIELD,NUMBER_PRODUCTS,TEXT_EMBEDDING_DIMENSION,'COSINE'\n",
    ")\n",
    "load_vectors(redis_conn,rooms_metadata,item_keywords_vectors,ITEM_KEYWORD_EMBEDDING_FIELD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "MODEL = 'gpt-3.5-turbo'\n",
    "openai_api_key = \"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\"\n",
    "\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name=MODEL,\n",
    "    temperature=0.3,\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "booking_prompt = PromptTemplate(\n",
    "    input_variables=[\"product_description\"],\n",
    "    template=\"Create comma seperated product keywords to perform a query on a airbnb dataset for this user input: \"\n",
    "             \"{product_description}\",\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=booking_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "userinput = input(\"Hey im a Roo(m)bot, how can i help you today? \")\n",
    "print(\"User:\", userinput)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "keywords = chain.run(userinput)\n",
    "\n",
    "topK=3\n",
    "#vectorize the query\n",
    "query_vector = model.encode(keywords).astype(np.float32).tobytes()\n",
    "\n",
    "#prepare the query\n",
    "q = Query(\n",
    "    f'*=>[KNN {topK} @{ITEM_KEYWORD_EMBEDDING_FIELD} $vec_param AS vector_score]'\n",
    ").sort_by('vector_score').paging(0,topK).return_fields(\n",
    "        'vector_score','property_type','name','amenities', 'city'\n",
    "    ).dialect(2)\n",
    "params_dict = {\"vec_param\": query_vector}\n",
    "\n",
    "#Execute the query\n",
    "results = redis_conn.ft().search(q, query_params = params_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_result_string = ''\n",
    "for product in results.docs:\n",
    "    full_result_string += ' '.join(\n",
    "        [\n",
    "            product.property_type, product.name, f\", amenities are:\", product.amenities,\" Located in city:\", product.city,\n",
    "            'ID of this booking is:', product.id,\n",
    "            \"\\n\\n\\n\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "BOOKING_AGENT_TEMPLATE = \"\"\"\n",
    "You are a room booking assistant. Be kind, detailed and try to sell the booking of the apartment to me. \n",
    "Present the three given queried search result in a nice way as answer to the user input.\n",
    "\n",
    "Collect entities after user confirms booking choice, we need these entities: booking_date_start, booking_date_end, name, surname. \n",
    "\n",
    "dont ask questions back! \n",
    "\n",
    "ALWAYS PROVIDE RESPONSE AS JSON STRING with keys:\n",
    "ANSWER, BOOKING_CONFIRMED, CONFIRMED_BOOKING_DATES, NAME and the ID of this.\n",
    "\n",
    "PLEASE ALWAYS HAVE THE JSON STRING PART\n",
    "\n",
    "\n",
    "\n",
    "{chat_history}\n",
    "Human: {user_msg}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "\n",
    "BOOKING_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"user_msg\"],\n",
    "    template=BOOKING_AGENT_TEMPLATE\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.8, openai_api_key=openai_api_key), \n",
    "    prompt=BOOKING_PROMPT, \n",
    "    verbose=False, \n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "\n",
    "answer = llm_chain.predict_and_parse(user_msg= f\"{full_result_string} ---\\n\\n {userinput}\")\n",
    "print(\"Bot:\", answer)\n",
    "time.sleep(0.5)\n",
    "\n",
    "while True:\n",
    "    follow_up = input(\"\")\n",
    "    print(\"User:\", follow_up)\n",
    "    answer = llm_chain.predict(\n",
    "        user_msg=follow_up\n",
    "    )\n",
    "    print(\"Bot:\", answer)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(re.findall(r'{.+}',answer)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_string = re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.loads(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOKING_AGENT_GET_INFO_TEMPLATE = \"\"\"\n",
    "You are a room booking assistant. Be nice and helpful and get these informations from the customer:\n",
    "BOOKING_START, BOOKING_END, FULL_NAME, CITY, BUDGET, GUEST_COUNT\n",
    "\n",
    "Example of how such conversation may go, only generate responses to the users last question:\n",
    "Human: Hi, I'm looking to book a room in CITY.\n",
    "assistant: Great, I can help with that. What's your budget?\n",
    "Human: I'm looking to spend around BUDGET euros per month.\n",
    "assistant: Okay, I've found a few options that fit your budget. Would you prefer to be in the city center or in a quieter area?\n",
    "Human: I want to be close to the main attractions.\n",
    "assistant: Got it. How many people will be staying in the room?\n",
    "Human: It's just me.\n",
    "\n",
    "\n",
    "Collect this information and ALWAYS ADD JSON STRING AT THE END OF THE RESPONSE AS JSON STRING with keys:\n",
    "BOOKING_START, BOOKING_END, FULL_NAME, CITY, BUDGET, GUEST_COUNT\n",
    "\n",
    "PLEASE ALWAYS HAVE THE JSON STRING PART\n",
    "\n",
    "{chat_history}\n",
    "Human: {user_msg}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "BOOKING_AGENT_TEMPLATE = \"\"\"\n",
    "You are a room booking assistant. Be kind, detailed and try to sell the booking of the apartment to me. \n",
    "Present the three given queried search result in a nice way as answer to the user input.\n",
    "\n",
    "Collect entities when user confirms booking choice, we need these entities: CONFIRMED_BOOKING_START, CONFIRMED_BOOKING_END, FULL_NAME\n",
    "\n",
    "ALWAYS ADD JSON STRING AT THE END OF THE RESPONSE, ALWAYS PROVIDE RESPONSE AS JSON STRING with keys:\n",
    "ANSWER, BOOKING_CONFIRMED, CONFIRMED_BOOKING_START, CONFIRMED_BOOKING_END, NAME and the ID of this.\n",
    "\n",
    "PLEASE ALWAYS HAVE THE JSON STRING PART\n",
    "\n",
    "\n",
    "{chat_history}\n",
    "Human: {user_msg}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "BOOKING_PROMPT_SELL = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"user_msg\"],\n",
    "    template=BOOKING_AGENT_TEMPLATE\n",
    ")\n",
    "\n",
    "BOOKING_PROMPT_ASK = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"user_msg\"],\n",
    "    template=BOOKING_AGENT_GET_INFO_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def booking_get_requirement_info(userinput):\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "    llm_chain = LLMChain(\n",
    "        llm=OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.8,\n",
    "                   openai_api_key=openai_api_key),\n",
    "        prompt=BOOKING_PROMPT_ASK,\n",
    "        verbose=False,\n",
    "        memory=memory,\n",
    "    )\n",
    "\n",
    "    answer = llm_chain.predict(user_msg=f\"{userinput}\")\n",
    "    return \"Bot:\", answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_get_requirement_info('I wanna book a room in Madrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
