{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yk3jkeRp3Dd-",
    "outputId": "2b352d3a-1598-4727-b25e-714fc705d0f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import redis\n",
    "import time\n",
    "\n",
    "from redis.commands.search.field import VectorField\n",
    "from redis.commands.search.field import TextField\n",
    "from redis.commands.search.field import TagField\n",
    "from redis.commands.search.query import Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "redis_conn = redis.Redis(\n",
    "  host='localhost',\n",
    "  port=6379,\n",
    "  password='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "00_n4VWH7FoB",
    "outputId": "f26daa8c-4af9-4def-d5ab-3197777fe2f9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_TEXT_LENGTH=512\n",
    "NUMBER_PRODUCTS=10000\n",
    "\n",
    "def auto_truncate(val):\n",
    "    return str(val)[:MAX_TEXT_LENGTH]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36494/902267153.py:1: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  room_data = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "room_data = pd.read_csv(\n",
    "    'app/data/airbnb.csv', delimiter=',', encoding_errors='ignore', on_bad_lines='skip',\n",
    "    converters={'bullet_point': auto_truncate,'amenities':auto_truncate,'name':auto_truncate}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "room_data = room_data[\n",
    "    [\n",
    "        'listing_id', 'name', 'host_id', 'host_since', 'host_location',\n",
    "       'host_is_superhost', 'host_total_listings_count',\n",
    "       'host_has_profile_pic', 'host_identity_verified', 'neighbourhood',\n",
    "       'district', 'city', 'property_type',\n",
    "       'room_type', 'accommodates', 'bedrooms', 'amenities', 'price',\n",
    "       'minimum_nights', 'maximum_nights', 'review_scores_rating',\n",
    "       'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "       'review_scores_checkin', 'review_scores_communication',\n",
    "       'review_scores_location', 'review_scores_value', 'instant_bookable'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# room_data = room_data.drop_duplicates(subset=['Area'], keep='first', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "room_data = room_data.sample(10000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "room_data[room_data.listing_id.str.contains('3444896')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get the first 1000 products with non-empty item keywords\n",
    "rooms_metadata = room_data.head(NUMBER_PRODUCTS).to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "gjouwTqF7ftf",
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m item_keywords \u001b[39m=\u001b[39m  [\n\u001b[1;32m      8\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(rooms_metadata[i][key]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhost_location\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneighbourhood\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mproperty_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mamenities\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m rooms_metadata\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     11\u001b[0m room_context \u001b[39m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     rooms_metadata[i][\u001b[39m'\u001b[39m\u001b[39mlisting_id\u001b[39m\u001b[39m'\u001b[39m]:\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(rooms_metadata[i][key]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhost_location\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneighbourhood\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mproperty_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mamenities\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m rooms_metadata\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m item_keywords_vectors \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39mencode(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m item_keywords]\n",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m item_keywords \u001b[39m=\u001b[39m  [\n\u001b[1;32m      8\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(rooms_metadata[i][key]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhost_location\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneighbourhood\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mproperty_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mamenities\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m rooms_metadata\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     11\u001b[0m room_context \u001b[39m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     rooms_metadata[i][\u001b[39m'\u001b[39m\u001b[39mlisting_id\u001b[39m\u001b[39m'\u001b[39m]:\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(rooms_metadata[i][key]) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhost_location\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneighbourhood\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mproperty_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mamenities\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m rooms_metadata\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m item_keywords_vectors \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39;49mencode(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m item_keywords]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    845\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    853\u001b[0m     embedding_output,\n\u001b[1;32m    854\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    855\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    856\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    857\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    858\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    859\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    860\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    861\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    862\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    863\u001b[0m )\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    865\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    528\u001b[0m         hidden_states,\n\u001b[1;32m    529\u001b[0m         attention_mask,\n\u001b[1;32m    530\u001b[0m         layer_head_mask,\n\u001b[1;32m    531\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    532\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    533\u001b[0m         past_key_value,\n\u001b[1;32m    534\u001b[0m         output_attentions,\n\u001b[1;32m    535\u001b[0m     )\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:411\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    400\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    401\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    409\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    412\u001b[0m         hidden_states,\n\u001b[1;32m    413\u001b[0m         attention_mask,\n\u001b[1;32m    414\u001b[0m         head_mask,\n\u001b[1;32m    415\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    416\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    418\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    420\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:338\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    329\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    330\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    337\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 338\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    339\u001b[0m         hidden_states,\n\u001b[1;32m    340\u001b[0m         attention_mask,\n\u001b[1;32m    341\u001b[0m         head_mask,\n\u001b[1;32m    342\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    343\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    344\u001b[0m         past_key_value,\n\u001b[1;32m    345\u001b[0m         output_attentions,\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    347\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    348\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:217\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    215\u001b[0m     value_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey(hidden_states))\n\u001b[1;32m    218\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    220\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "#from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
    "\n",
    "\n",
    "item_keywords =  [\n",
    "    ', '.join(str(rooms_metadata[i][key]) for key in ['name', 'host_location', 'neighbourhood', 'property_type', 'amenities', 'price'])\n",
    "    for i in rooms_metadata.keys()\n",
    "]\n",
    "room_context = {\n",
    "    rooms_metadata[i]['listing_id']:', '.join(str(rooms_metadata[i][key]) for key in ['name', 'host_location', 'neighbourhood', 'property_type', 'amenities', 'price'])\n",
    "    for i in rooms_metadata.keys()\n",
    "}\n",
    "item_keywords_vectors = [model.encode(sentence) for sentence in item_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aluguel temporada olimpÃƒ\\xadadas, Rio de Janeiro, State of Rio de Janeiro, Brazil, Barra da Tijuca, Entire apartment, [\"Elevator\", \"Dedicated workspace\", \"Kitchen\", \"Washer\", \"Air conditioning\", \"Pool\", \"Long term stays allowed\", \"Gym\", \"Essentials\", \"Dryer\", \"Iron\", \"TV\", \"Wifi\", \"Hot tub\", \"Fire extinguisher\", \"Hangers\", \"Heating\", \"Free parking on premises\", \"Cable TV\"], 3000'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_keywords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "Iw7rlppY8f3a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'listing_id': 8325748,\n",
       " 'name': 'Valmy',\n",
       " 'host_id': 43888027,\n",
       " 'host_since': '2015-09-10',\n",
       " 'host_location': 'Paris, Ile-de-France, France',\n",
       " 'host_is_superhost': 'f',\n",
       " 'host_total_listings_count': 1.0,\n",
       " 'host_has_profile_pic': 't',\n",
       " 'host_identity_verified': 't',\n",
       " 'neighbourhood': 'Palais-Bourbon',\n",
       " 'district': nan,\n",
       " 'city': 'Paris',\n",
       " 'property_type': 'Entire apartment',\n",
       " 'room_type': 'Entire place',\n",
       " 'accommodates': 2,\n",
       " 'bedrooms': 1.0,\n",
       " 'amenities': '[\"Essentials\", \"Stove\", \"Hot water\", \"Hangers\", \"Smoke alarm\", \"Wifi\", \"Luggage dropoff allowed\", \"Long term stays allowed\", \"Dedicated workspace\", \"Host greets you\", \"TV\", \"Iron\", \"Dryer\", \"Refrigerator\", \"Oven\", \"Dishes and silverware\", \"Private entrance\", \"Heating\", \"Ethernet connection\", \"Kitchen\", \"Hair dryer\", \"Coffee maker\", \"Washer\", \"Microwave\", \"Cooking basics\", \"Cable TV\", \"Paid parking off premises\", \"Shampoo\", \"Shower gel\", \"Bed linens\", \"Carbon monoxide alarm\", \"Elevator\"]',\n",
       " 'price': 140,\n",
       " 'minimum_nights': 3,\n",
       " 'maximum_nights': 1125,\n",
       " 'review_scores_rating': 98.0,\n",
       " 'review_scores_accuracy': 10.0,\n",
       " 'review_scores_cleanliness': 10.0,\n",
       " 'review_scores_checkin': 10.0,\n",
       " 'review_scores_communication': 10.0,\n",
       " 'review_scores_location': 10.0,\n",
       " 'review_scores_value': 10.0,\n",
       " 'instant_bookable': 'f'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item_keywords_vectors)\n",
    "len(rooms_metadata)\n",
    "# Check one of the products\n",
    "rooms_metadata[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "ayGF0v-w8mxy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import redis\n",
    "\n",
    "from redis.commands.search.field import VectorField\n",
    "from redis.commands.search.field import TextField\n",
    "from redis.commands.search.query import Query\n",
    "\n",
    "\n",
    "def get_redis_connector(host='localhost', port=6379, password=''):\n",
    "    logging.warning('Connecting to redis...')\n",
    "    try:\n",
    "        return redis.Redis(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            password=password\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(e, exc_info=True)\n",
    "\n",
    "\n",
    "def get_booking_query(topK):\n",
    "    logging.warning('Querying redis...')\n",
    "    return Query(\n",
    "        f'*=>[KNN {topK} @item_keyword_vector $vec_param AS vector_score]'\n",
    "    ).sort_by('vector_score').paging(0, topK).return_fields(\n",
    "        'vector_score', 'property_type', 'name', 'amenities', 'city'\n",
    "    ).dialect(2)\n",
    "\n",
    "\n",
    "def create_flat_index(redis_conn, vector_field_name, number_of_vectors, vector_dimensions=512, distance_metric='L2'):\n",
    "    logging.warning(f'Generating vector index for `{number_of_vectors}` records.')\n",
    "    redis_conn.ft().create_index([\n",
    "        VectorField(\n",
    "            vector_field_name, \"FLAT\",\n",
    "            {\n",
    "                \"TYPE\": \"FLOAT32\",\n",
    "                \"DIM\": vector_dimensions,\n",
    "                \"DISTANCE_METRIC\": distance_metric,\n",
    "                \"INITIAL_CAP\": number_of_vectors,\n",
    "                \"BLOCK_SIZE\": number_of_vectors\n",
    "            }\n",
    "        ),\n",
    "        TextField(\"property_type\", as_name='property_type'),\n",
    "        TextField(\"name\", as_name='name'),\n",
    "        TextField(\"amenities\", as_name='amenities'),\n",
    "        TextField(\"city\", as_name='city')\n",
    "    ])\n",
    "\n",
    "\n",
    "def load_vectors(client, product_metadata, vector_dict, vector_field_name):\n",
    "    logging.warning('Loading vectors to redis...')\n",
    "    p = client.pipeline(transaction=False)\n",
    "    for index in product_metadata.keys():\n",
    "        # hash key\n",
    "        key = 'listing_id: ' + str(product_metadata[index]['listing_id'])\n",
    "\n",
    "        # hash values\n",
    "        item_metadata = product_metadata[index]\n",
    "        item_keywords_vector = vector_dict[index].astype(np.float32).tobytes()\n",
    "        item_metadata[vector_field_name] = item_keywords_vector\n",
    "\n",
    "        # HSET\n",
    "        p.hset(key, mapping=item_metadata)\n",
    "\n",
    "    p.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "lcjo2rXb8pT9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Indexing + 10000 products\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ITEM_KEYWORD_EMBEDDING_FIELD='item_keyword_vector'\n",
    "TEXT_EMBEDDING_DIMENSION=768\n",
    "NUMBER_PRODUCTS=10000\n",
    "\n",
    "print ('Loading and Indexing + ' +  str(NUMBER_PRODUCTS) + ' products')\n",
    "\n",
    "#flush all data\n",
    "redis_conn.flushall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generating vector index for `10000` records.\n",
      "WARNING:root:Loading vectors to redis...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create flat index & load vectors\n",
    "create_flat_index(\n",
    "    redis_conn, ITEM_KEYWORD_EMBEDDING_FIELD,NUMBER_PRODUCTS,TEXT_EMBEDDING_DIMENSION,'COSINE'\n",
    ")\n",
    "load_vectors(redis_conn,rooms_metadata,item_keywords_vectors,ITEM_KEYWORD_EMBEDDING_FIELD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "MODEL = 'gpt-3.5-turbo'\n",
    "openai_api_key = \"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\"\n",
    "\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name=MODEL,\n",
    "    temperature=0.3,\n",
    "    openai_api_key=openai.api_key\n",
    ")\n",
    "\n",
    "booking_prompt = PromptTemplate(\n",
    "    input_variables=[\"product_description\"],\n",
    "    template=\"Create comma seperated product keywords to perform a query on a airbnb dataset for this user input: \"\n",
    "             \"{product_description}\",\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hey im a Roo(m)bot, how can i help you today?  apartment balcony madrid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: apartment balcony madrid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Querying redis...\n"
     ]
    }
   ],
   "source": [
    "userinput = input(\"Hey im a Roo(m)bot, how can i help you today? \")\n",
    "print(\"User:\", userinput)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "keywords = chain.run(userinput)\n",
    "\n",
    "topK=3\n",
    "#vectorize the query\n",
    "query_vector = model.encode(keywords).astype(np.float32).tobytes()\n",
    "\n",
    "#prepare the query\n",
    "q = get_booking_query(topK)\n",
    "params_dict = {\"vec_param\": query_vector}\n",
    "\n",
    "#Execute the query\n",
    "results = redis_conn.ft().search(q, query_params = params_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result{0 total, docs: []}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*=>[KNN 3 @item_keyword_vector $vec_param AS vector_score]',\n",
       " 'RETURN',\n",
       " 5,\n",
       " 'vector_score',\n",
       " 'property_type',\n",
       " 'name',\n",
       " 'amenities',\n",
       " 'city',\n",
       " 'SORTBY',\n",
       " 'vector_score',\n",
       " 'ASC',\n",
       " 'DIALECT',\n",
       " 2,\n",
       " 'LIMIT',\n",
       " 0,\n",
       " 3]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_result_string = ''\n",
    "for product in results.docs:\n",
    "    full_result_string += ' '.join(\n",
    "        [\n",
    "            product.property_type, product.name, f\", amenities are:\", product.amenities,\" Located in city:\", product.city,\n",
    "            'ID of this booking is:', product.id,\n",
    "            \"\\n\\n\\n\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "BOOKING_AGENT_TEMPLATE = \"\"\"\n",
    "You are a room booking assistant. Be kind, detailed and try to sell the booking of the apartment to me. \n",
    "Present the three given queried search result in a nice way as answer to the user input.\n",
    "\n",
    "Collect entities after user confirms booking choice, we need these entities: booking_date_start, booking_date_end, name, surname. \n",
    "\n",
    "dont ask questions back! \n",
    "\n",
    "ALWAYS PROVIDE RESPONSE AS JSON STRING with keys:\n",
    "ANSWER, BOOKING_CONFIRMED, CONFIRMED_BOOKING_DATES, NAME and the ID of this.\n",
    "\n",
    "PLEASE ALWAYS HAVE THE JSON STRING PART\n",
    "\n",
    "\n",
    "\n",
    "{chat_history}\n",
    "Human: {user_msg}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "\n",
    "BOOKING_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"user_msg\"],\n",
    "    template=BOOKING_AGENT_TEMPLATE\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.8, openai_api_key=openai_api_key), \n",
    "    prompt=BOOKING_PROMPT, \n",
    "    verbose=False, \n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "\n",
    "answer = llm_chain.predict_and_parse(user_msg= f\"{full_result_string} ---\\n\\n {userinput}\")\n",
    "print(\"Bot:\", answer)\n",
    "time.sleep(0.5)\n",
    "\n",
    "while True:\n",
    "    follow_up = input(\"\")\n",
    "    print(\"User:\", follow_up)\n",
    "    answer = llm_chain.predict(\n",
    "        user_msg=follow_up\n",
    "    )\n",
    "    print(\"Bot:\", answer)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(re.findall(r'{.+}',answer)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_string = re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.loads(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOKING_AGENT_GET_INFO_TEMPLATE = \"\"\"\n",
    "You are a room booking assistant. Be nice and helpful and get these informations from the customer:\n",
    "BOOKING_START, BOOKING_END, FULL_NAME, CITY, BUDGET, GUEST_COUNT\n",
    "\n",
    "Example of how such conversation may go, only generate responses to the users last question:\n",
    "Human: Hi, I'm looking to book a room in CITY.\n",
    "assistant: Great, I can help with that. What's your budget?\n",
    "Human: I'm looking to spend around BUDGET euros per month.\n",
    "assistant: Okay, I've found a few options that fit your budget. Would you prefer to be in the city center or in a quieter area?\n",
    "Human: I want to be close to the main attractions.\n",
    "assistant: Got it. How many people will be staying in the room?\n",
    "Human: It's just me.\n",
    "\n",
    "\n",
    "Collect this information and ALWAYS ADD JSON STRING AT THE END OF THE RESPONSE AS JSON STRING with keys:\n",
    "BOOKING_START, BOOKING_END, FULL_NAME, CITY, BUDGET, GUEST_COUNT\n",
    "\n",
    "PLEASE ALWAYS HAVE THE JSON STRING PART\n",
    "\n",
    "{chat_history}\n",
    "Human: {user_msg}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "BOOKING_AGENT_TEMPLATE = \"\"\"\n",
    "You are a room booking assistant. Be kind, detailed and try to sell the booking of the apartment to me. \n",
    "Present the three given queried search result in a nice way as answer to the user input.\n",
    "\n",
    "Collect entities when user confirms booking choice, we need these entities: CONFIRMED_BOOKING_START, CONFIRMED_BOOKING_END, FULL_NAME\n",
    "\n",
    "ALWAYS ADD JSON STRING AT THE END OF THE RESPONSE, ALWAYS PROVIDE RESPONSE AS JSON STRING with keys:\n",
    "ANSWER, BOOKING_CONFIRMED, CONFIRMED_BOOKING_START, CONFIRMED_BOOKING_END, NAME and the ID of this.\n",
    "\n",
    "PLEASE ALWAYS HAVE THE JSON STRING PART\n",
    "\n",
    "\n",
    "{chat_history}\n",
    "Human: {user_msg}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "BOOKING_PROMPT_SELL = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"user_msg\"],\n",
    "    template=BOOKING_AGENT_TEMPLATE\n",
    ")\n",
    "\n",
    "BOOKING_PROMPT_ASK = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"user_msg\"],\n",
    "    template=BOOKING_AGENT_GET_INFO_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def booking_get_requirement_info(userinput):\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "    llm_chain = LLMChain(\n",
    "        llm=OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.8,\n",
    "                   openai_api_key=openai_api_key),\n",
    "        prompt=BOOKING_PROMPT_ASK,\n",
    "        verbose=False,\n",
    "        memory=memory,\n",
    "    )\n",
    "\n",
    "    answer = llm_chain.predict(user_msg=f\"{userinput}\")\n",
    "    return \"Bot:\", answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_get_requirement_info('I wanna book a room in Madrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
